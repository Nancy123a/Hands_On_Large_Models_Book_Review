{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8028f594-be20-489a-a6ce-739f514ebeac",
   "metadata": {},
   "source": [
    "## Chapter 3: Looking Inside Large Language Models\n",
    "\n",
    "- The model does not generate the text all in one operation; it actually generates one token at a time.\n",
    "- Each token generation step is one forward pass through the mode\n",
    "- After each token generation, we tweak the input prompt for the next generation step by appending the output token to the end of the input prompt.\n",
    "-  Text generation LLMs being called autoregressive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526ae2b2-8345-445a-968d-2c9a0f36e7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccbc8ac919f47b090368d0bd482e615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49aaa15b-c259-4003-8967-1f0a180095f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mention the steps you're taking to prevent it in the future.\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86026a37-5907-4bce-b25f-3e26d64d967c",
   "metadata": {},
   "source": [
    "- It stopped abruptly because it reached the token limit we established by setting max_new_tokens to 50 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc2db64-99b0-4c8c-b324-c92e106033c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with max_new_tokens=100 (overriding default):\n",
      "\n",
      "\n",
      "# Answer\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating with max_new_tokens=100 (overriding default):\")\n",
    "output_100 = generator(\"What is the capital of France?\", max_new_tokens=100)\n",
    "print(output_100[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48327318-108f-4bb7-a91b-43df35174387",
   "metadata": {},
   "source": [
    "## The Components of the Forward Pass\n",
    "- Tokenizers break down the text into a sequence of token IDs that then become the input to the model\n",
    "- The tokenizer is followed by the neural network: a stack of Transformer blocks that do all of the processing. That stack is then followed by the LM head, which translates the output of the stack into probability scores for what the most likely next token is.\n",
    "  \n",
    "![Alt text for the image](images/transformer_blocks.png)\n",
    "\n",
    "- The tokenizer contains a table of tokensâ€”the tokenizerâ€™s vocabulary. The model has a vector representation associated with each of these tokens in the vocabulary (token embeddings)\n",
    "\n",
    "![Alt text for the image](images/tokenzier.png)\n",
    "\n",
    "- The flow of the computation follows the direction of the arrow from top to bottom. For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probability distribution for the next token.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe42526-1bd0-4e9a-8322-f79c65e8e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d94b5-a52e-4496-95bf-ce6398bb28ed",
   "metadata": {},
   "source": [
    "# Choosing a Single Token from the Probability Distribution (Sampling Decoding)\n",
    "\n",
    "- At the end of processing, the output of the model is a probability score for each token in the vocabulary.\n",
    "- The method of choosing a single token from the probability distribution is called the decoding strategy.\n",
    "- The easiest decoding strategy would be to always pick the token with the highest probability score.\n",
    "- In practice, this doesnâ€™t tend to lead to the best outputs for most use cases. A better approach is to add some randomness and sometimes choose the second or third highest probability token.\n",
    "- Choosing the highest scoring token every time is called greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e848e36a-58f3-4d58-9f6c-01eccbfd6d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 450, 7483,  310, 3444,  338]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'The capital of France is'\n",
    "\n",
    "## Tokenize the input prompt\n",
    "input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6cd5f0-49e5-4b53-9438-968fe0c062ce",
   "metadata": {},
   "source": [
    "- Each of the word in prompt is encoded and its in the input_ids tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42573007-3330-48a6-a7f7-ef7634eafa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=input_ids.to('cuda') ##send to gpu\n",
    "## Get the output of the model before the lm_head\n",
    "model_output=model.model(input_ids)\n",
    "## Get the output of lm head\n",
    "lm_head_output=model.lm_head(model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22e7d079-5398-4ea7-85b2-47cf245ba9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b07e37ba-c78a-47d0-b3df-71967e078209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32064])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339cf6c-b0c1-438c-a3ea-84909d5e7dcb",
   "metadata": {},
   "source": [
    "-  batch of one input string, containing five tokens, each of them represented by a vector of size 3,072 corresponding to the output vectors after the stack of Transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b884d151-bdda-4492-9e3e-33a234293a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pick the best highest probbaility token (greedy decoding)\n",
    "## access the token probability scores for the last generated token\n",
    "##  which uses the index 0 across the batch dimension; the index â€“1 gets us the last token in the sequence\n",
    "token_id=lm_head_output[0,-1].argmax(-1)\n",
    "tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7177b8-5bd7-47d8-9805-25f6935c87d7",
   "metadata": {},
   "source": [
    "## Parallel Token Processing and Context Size\n",
    "- Tranformers can provide parrallel processing\n",
    "- Tokenizer will break down the text into tokens. Each of these input tokens then flows through its own computation path\n",
    "- Current Transformer models have a limit for how many tokens they can process at once. That limit is called the modelâ€™s context length. A model with 4K context length can only process 4K tokens and would only have 4K of these streams.\n",
    "- Each of the token streams starts with an input vector (the embedding vector and some positional information.\n",
    "  \n",
    "![Alt text for the image](images/floe.png)\n",
    "\n",
    "- For text generation, only the output result of the last stream is used to predict the next token. That output vector is the only input into the LM head as it calculates the probabilities of the next token.\n",
    "- The calculations of the previous streams are required and used in calculating the final stream. Yes, weâ€™re not using their final output vector, but we use earlier outputs (in each Transformer block) in the Transformer blockâ€™s attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a47bd6-b8f8-4c9b-9d94-60c8c472c343",
   "metadata": {},
   "source": [
    "## Speeding Up Generation by Caching Keys and Values\n",
    "- Recall that when generating the second token, we simply append the output token to the input and do another forward pass through the model.\n",
    "- If we give the model the ability to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), we no longer need to repeat the calculations of the previous streams. This time the only needed calculation is for the last stream (key-value cache and it speed up of generation process).\n",
    "- In Hugging Face Transformers, cache is enabled by default. We can disable it by setting use_cache to False. We can see the difference in speed by asking for a long generation, and timing the generation with and without caching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce3eb700-1cee-468d-ade5-b3cf5de81030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "333fdd7e-4d2f-464e-85e2-774b6536ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24 s Â± 102 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%timeit -n 1\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=100,\n",
    "  use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f1254e5-219f-4e25-8476-af3dbff0f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.74 s Â± 445 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=100,\n",
    "  use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a3b41-ae32-402f-80a9-7b47daea5e0a",
   "metadata": {},
   "source": [
    "- Enabling the cache of key and values save time and make the execution process faster\n",
    "\n",
    "## Inside the transformer\n",
    "\n",
    "- Transformer consist of a series of transformers (6 in the origional paper - Attention is all you need)\n",
    "- Each block process its inputs, then passes the results of its processing to the next block\n",
    "- Each transformer block is made up of attention layer (incorporate relevant info from other tokens and positions) and FFN (majority of the model's processing capacity)\n",
    "\n",
    "![Alt text for the image](images/transformer.png)\n",
    "\n",
    "- Attention is a mechanism that helps the model incorporate context as itâ€™s processing a\n",
    "specific token. for example referring it to dog in â€œThe dog chased the squirrel because it\"\n",
    "\n",
    "- A way to score how relevant each of the previous input tokens are to the current\n",
    "token being processed (last token processed).\n",
    "\n",
    "- To give the Transformer more extensive attention capability, the attention mechanism is duplicated and executed multiple times in parallel. Each of these parallel applications of attention is conducted into an attention head.\n",
    "\n",
    "- Attention starts by multiplying the inputs by the projection matrices to create three new matrices. These are called the queries, keys, and values matrices. These matrices contain the information of the input tokens projected to three different spaces that help carry out the two steps of attention:\n",
    "\n",
    "Relevence Scoring and Combining information\n",
    "\n",
    "![Alt text for the image](images/key_value_query.png)\n",
    "\n",
    "- ** Self attention: Relevance Scoring**\n",
    "\n",
    "In a generative Transformer, weâ€™re generating one token at a time. This means weâ€™re processing one position at a time. So the attention mechanism here is only concerned with this one position, and how information from other positions can be pulled in to inform this position.\n",
    "\n",
    "- The relevance scoring step of attention is conducted by multiplying the query vector of the current position with the keys matrix. This produces a score stating how relevant each previous token is. Passing that by a softmax operation normalizes these scores so they sum up to 1.\n",
    "\n",
    "![Alt text for the image](images/query_key_multi.png)\n",
    "\n",
    "**Self-attention: Combining information**\n",
    "\n",
    "-  Now that we have the relevance scores, we multiply the value vector associated with each token by that tokenâ€™s score. Summing up those resulting vectors produces the output of this attention step.\n",
    "\n",
    "\n",
    "![Alt text for the image](images/key_value_sum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abea79c-a121-4757-8b1d-d3afa3d415c5",
   "metadata": {},
   "source": [
    "## Recent Improvements to the Transformer Architecture\n",
    "\n",
    "**Local/sparse attention**\n",
    "\n",
    "-  Sparse attention limits the context of previous tokens that the model can attend to.\n",
    "\n",
    "![Alt text for the image](images/attention_types.png)\n",
    "\n",
    "- **Full attention**: In full attention, every token in a sequence attends to every other token in that same sequence. This means that for each token, its \"query\" is compared against the \"keys\" of all other tokens (including itself) to calculate attention scores. These scores are then used to create a weighted sum of the \"values\" from all tokens (quadratic computational complexity with respect to the sequence length (N). If you have a sequence of length N, the attention mechanism requires O(N^2) computations and memory.\n",
    "          1) Adventages: Global Context and simplicity\n",
    "          2) Disadvantages: Scalability issue and high resource consumption.\n",
    "\n",
    "- **Sparse attention:**  Sparse attention is an optimization technique designed to reduce the computational and memory demands of full attention, particularly for long sequences. Instead of every token attending to all other tokens, sparse attention introduces \"structured sparsity\" in the attention matrix. This means that each token only attends to a subset of other tokens. (reduce the complexity from quadratic (O(N^2)) to something more efficient, often linear (O(N)) or nearly linear (O(N radical(N)), O(NlogN)).\n",
    "\n",
    "**Approaches to sparsity**:\n",
    "- **Local Attention** (Windowed/Sliding Attention): Tokens only attend to other tokens within a fixed-size local window around them. This is efficient but might miss very long-range dependencies.\n",
    "- **Fixed Patterns**: Predetermined patterns of attention, such as: **Strided Attention**: Tokens attend to others at regular intervals (e.g., every ð‘˜-th token). **Fixed Attention**: Tokens in a block attend to other tokens within the same block, or certain \"global\" tokens.\n",
    "- **Random Attention**: Each token attends to a fixed number of randomly selected tokens\n",
    "- **Global Attention**: Some designated \"global\" tokens can attend to all other tokens, acting as intermediaries for broader communication.\n",
    "- **Learnable/Dynamic Sparsity**: The model learns which connections are most important and prunes the less important ones or routes queries to specific key subsets (e.g., Routing Transformers, Native Sparse Attention).\n",
    "- **Content-Based Sparsity**: Attention is focused on the most relevant tokens based on their content, rather than just their position.\n",
    "- **Benefits:**\n",
    "-  **Scalability**, Significantly reduces computational cost and memory footprint, enabling the processing of much longer sequences than full attention. This is crucial for tasks like long-document summarization, time-series forecasting, and handling large-scale generative models.\n",
    "-  **Efficiency**: Faster training and inference due to fewer computations.\n",
    "-  **Drawbacks**: potential loss info, complexity of design, implementation challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf080171-4fa4-4b88-a126-7fc06b96c6e9",
   "metadata": {},
   "source": [
    "** Multi-query and grouped-query attention **\n",
    "\n",
    "- Presented in research paper: â€œGQA: Training generalized multi-query transformer models from multi-head checkpointsâ€\n",
    "  \n",
    "![Alt text for the image](images/multi_head.png)\n",
    "\n",
    "1 - **Multi query:** In standard Multi-Head Attention, each \"head\" of attention has its own independent set of learnable weight matrices for Queries (Q), Keys (K), and Values (V). This means if you have 'H' attention heads, you have 'H' sets of (Q, K, V) projection matrices.\n",
    "\n",
    "- **Benefit:** This allows the model to learn diverse relationships and focus on different aspects of the input simultaneously, contributing to its strong performance\n",
    "\n",
    "- **Drawbacks:**\n",
    "\n",
    "- **High Memory Bandwidth**: During inference, particularly in autoregressive decoding (generating text token by token), the Key and Value (KV) cache for previous tokens grows with sequence length. With 'H' separate K and V matrices, this cache can become very large, leading to significant memory consumption and bandwidth bottlenecks.\n",
    "\n",
    "- **Slower Inference**: Loading and processing these large KV caches for each head can slow down the generation process.\n",
    "\n",
    "2 - **Multi-Query Attention (MQA)**\n",
    "\n",
    "- Mechanism: MQA is an aggressive optimization where all attention heads share the same single set of Key (K) and Value (V) projection matrices. Only the Query (Q) matrices remain separate for each head.\n",
    "\n",
    "- **Benefits**\n",
    "\n",
    "- Significantly Reduced KV Cache Size: Since all query heads use the same K and V, the KV cache size is drastically reduced (by a factor of 'H', the number of heads). This leads to much lower memory consumption and memory bandwidth requirements.\n",
    "\n",
    "\n",
    "- Faster Inference: The smaller KV cache means less data needs to be loaded from memory, resulting in substantial speedups during inference. This is particularly beneficial for long sequence generation.\n",
    "\n",
    "- **Drawbacks**\n",
    "\n",
    "- Potential Quality Degradation: The major trade-off is a potential reduction in model quality or performance. Forcing all query heads to look at the same K and V information might limit the model's ability to learn diverse and intricate relationships, as each head can't specialize as much. This \"information bottleneck\" can lead to a slight drop in accuracy on some tasks.\n",
    "\n",
    "\n",
    "- Training Instability: Some studies have noted that MQA can sometimes lead to training instability compared to MHA.\n",
    "\n",
    "3- **Grouped-query attention**:  GQA strikes a balance between MHA and MQA. Instead of sharing a single K and V set across all query heads (like MQA), GQA divides the query heads into a smaller number of groups, and each group shares a single set of Key (K) and Value (V) projection matrices.\n",
    "\n",
    "- **Benefits**\n",
    "\n",
    "- **Balances Speed and Quality:** GQA achieves much of the speedup of MQA by reducing the KV cache size (though not as much as MQA), while retaining more of the representational power and quality of MHA. It reduces memory bandwidth and latency significantly compared to MHA, but typically performs better than MQA in terms of model quality.\n",
    "\n",
    "- **Flexible Trade-off:** The number of groups 'G' is a tunable hyperparameter, allowing developers to explicitly choose the trade-off between inference speed and model quality.\n",
    "\n",
    "- **Widely Adopted:** Many modern LLMs (e.g., Llama 2 70B, Mistral 7B) use GQA because of its favorable balance.\n",
    "\n",
    "- Improve inference scalability of larger models by reducing the size of the matrices involved.\n",
    "\n",
    "![Alt text for the image](images/attn1.png)\n",
    "\n",
    "![Alt text for the image](images/attn2.png)\n",
    "\n",
    "![Alt text for the image](images/attn3.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
